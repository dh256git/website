---
title: "Umwelt: Accessible structured editing of multi-modal data representations"
slug: umwelt-chi-2024
authors: J. Zong; I. Pedraza Pineros; M. K. Chen; D. Hajas; A. Satyanarayan
venue: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (pp. 46:1–46:20). Association for Computing Machinery
year: 2024
doi: 10.1145/3613904.3641996
publisher_url: https://dl.acm.org/doi/10.1145/3613904.3641996
pdf_url: /assets/papers/umwelt-chi-2024.pdf
html_url: https://dl.acm.org/doi/10.1145/3613904.3641996
tags:
  - representational-productivity-tools
  - cross-modal-ux-collaboration
abstract: >
  We present Umwelt, an authoring environment for interactive multimodal data representations. In contrast to prior approaches, which center the visual modality, Umwelt treats visualization, sonification, and textual description as coequal representations: they are all derived from a shared abstract data model, such that no modality is prioritized over the others. To simplify specification, Umwelt evaluates a set of heuristics to generate default multimodal representations that express a dataset’s functional relationships. To support smoothly moving between representations, Umwelt maintains a shared query predicated that is reified across all modalities — for instance, navigating the textual description also highlights the visualization and filters the sonification. In a study with 5 blind / low-vision expert users, we found that Umwelt’s multimodal representations afforded complementary overview and detailed perspectives on a dataset, allowing participants to fluidly shift between task- and representation-oriented ways of thinking.
---
